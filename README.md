# udacity-data-engineer-dwh: Sparkify ETL Pipeline

This projects implements a data warehouse using Redshift/S3 and a pipeline to ingest files into the database. 

Two Datasets are used:
1. A song data, which is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/)
2. A log dataset, which consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above


Both files are stored in S3 bucket `'s3://udacity-dend` and are ingested into fact and dimensional tables as described in file `sql_queries.py`

The project consists of the following files:
- redshift.py to manage bringup and tear down of a redshift cluster
- create_tables.py: to create the Redshift fact and dimension tables
- etl.py: to run the ETL extracting facts and dimension date from the S3 files

# Installation

Install prerequiristes for postgresql as defined in https://www.psycopg.org/docs/install.html

if you find an issue with -lssl, try running the following:
`env LDFLAGS="-I/usr/local/opt/openssl/include -L/usr/local/opt/openssl/lib" pip install psycopg2`
per [stackflow](https://stackoverflow.com/questions/26288042/error-installing-psycopg2-library-not-found-for-lssl)

Create virtual environments: 

`python3 -m venv venv`

Activate the environment: 

`source venv/bin/activate`

Install python packages: 

`pip3 install -r requirements.txt`


## Redshift Cluster

This section assumes an admin role has been created in AWS and that AWS `KEY` and `SECRET` are stored in the `aws.cfg` file.

### Create the cluster
Before starting the Pipeline, create a redshift cluster using the following command

`python redshift.py --cmd create`

This will create a Redshift cluster and assign it an S3 Read-Only role.

This script relies on credential information from `aws.cfg` as well as AWS cluster configuration information from file `dhw.cfg`

The command will create a `cluster.cfg` file containing both the redshift `hostname` as well as the `ARN`
for the role created, which will be used by the pipeline scripts.

### Delete the Redshift cluster

To delete the cluster, run the following command: 

`python redshift.py --cmd delete`


##  Running The Pipeline

There are 2 steps:

1) Create the Redshift tables, including staging as well as fact and dimension tables:

`python create_tables`

2) Run the ETL pipeline: 

`python etl.py`


# Analysis

After running the pipeline, we are able to do some analysis on the `songplay` table. 

For example, we can find the top 5 power users with the following query:

~~~ sql
WITH top_users AS (
    SELECT user_id, COUNT(*) AS cnt
    FROM songplay
    GROUP BY user_id
    ORDER BY cnt DESC
    LIMIT 5
)
SELECT users.first_name, 
       users.last_name, 
       top_users.cnt
  FROM top_users
 INNER JOIN users
       on users.user_id = top_users.user_id
 ORDER BY cnt DESC
~~~~

and we get:

| first_name | last_name | cnt |
| ------------- |:-------------:|:-------------:|
| Chloe | Cuevas | 689 |
| Tegan | Levine | 665 |
| Kate | Harrell | 557 |
| Lily | Koch | 463 | 
| Aleena | Kirby | 397 |

We can also look for the top 5 most popular locations where songs are played, using the following query:

~~~ sql
SELECT location, 
       count(*) as cnt 
  FROM songplay
 GROUP BY location 
 ORDER BY cnt DESC 
 LIMIT 5
~~~~

| location | count |
| ------------- |:-------------:|
| San Francisco-Oakland-Hayward, CA | 691
| Portland-South Portland, ME | 665
| Lansing-East Lansing, MI | 557
| Chicago-Naperville-Elgin, IL-IN-WI | 475
| Atlanta-Sandy Springs-Roswell, GA | 456

